#!/usr/bin/env python3
#
# Use this script to output the detailed status of a 
# Cromwell workflow run on the MacCoss lab
# Cromwell server, m002.grid.gs.washington.edu.
#


import requests
import logging
import datetime
import argparse
# import subprocess
import sys
import os
# import time
import json
import pytz
# import shutil

logging.basicConfig(
    level=logging.INFO, format="%(asctime)s: %(levelname)s: %(message)s"
)
LOGGER = logging.getLogger()


def convert_to_local_time(datetime_str):
    # Convert datetime string to local_tz timezone. This function
    # assumes that the incoming datetime string is using the UTC
    # timezone. This is a good assumption for Cromwell as Cromwell
    # writes all dates in Metadata in UTC by default.
    try:
        local_tz = pytz.timezone('America/Los_Angeles')

        # Create datetime object from string
        dto = datetime.datetime.strptime(datetime_str, "%Y-%m-%dT%H:%M:%S.%fZ")

        # datetime strings generated by Cromwell are always in UTC. Add the
        # timezone to the datetime object
        utc_tz = pytz.timezone('UTC')
        dto_utc = utc_tz.localize(dto)

        # Convert to local_tz
        dto_local = dto_utc.astimezone(local_tz)
    except ValueError:
        # The datetime is in the wrong format. Will use inst["start"] string
        LOGGER.warn(
            "There was a problem converting datetime to local timezone ",
            "The datetime string from the metadata will be used. This ",
            "datetime is in UTC timezone"
        )
        dto_local = datetime_str

    return dto_local


#
# Variables
#
cromwell_server_url = "http://m002.grid.gs.washington.edu:8000"
start_date = "{:%Y%m%d-%H%M%z}".format(datetime.datetime.now())

# Values for testing locally
# cromwell_server = "127.0.0.1"
# cromwell_scp_port = "8222"
# cromwell_scp_key = "/Users/bconn/.ssh/ssh_cromwell_rsa"
cromwell_server_url = "http://127.0.0.1:8000"
# cromwell_bin = "~/bin/cromwell/cromwell-47.jar"

#
# Parse our arguments
#
parser = argparse.ArgumentParser(
    description="Check status of Workflow on MacCoss Lab Cromwell Server"
)
parser.add_argument("--id", help="Workflow ID")

args = parser.parse_args()

# Check for required arguments
if not args.id:
    LOGGER.error(
        "You must specify a workflow ID on the command line. "
        "add the --id option and try again"
    )
    parser.print_usage()
    sys.exit(1)


#
# Determine Workflow status
#
# Check the status of the workflow
workflow_id = args.id
status_url = "{}/api/workflows/v1/{}/status".format(
    cromwell_server_url, workflow_id
)
status_headers = {"accept": "application/json"}

workflow_status = None
try:
    resp = requests.get(status_url, headers=status_headers)
    resp.raise_for_status()
except requests.exceptions.HTTPError as errh:
    # The status URL is not immediately available after submission of
    # workflow. Until is available, the server will respond with a 404
    # We will wait up to 1 minute for the status URL to be available.
    if resp.status_code == 404:
        LOGGER.error("Workflow is not available on server...")
        LOGGER.error("Wait a minute and run this command again.")
        sys.exit(1)
    else:
        LOGGER.error(
            "Status request failed with HTTP Error %s. URL of request was %s",
            errh,
            status_url,
        )
        sys.exit(1)
except requests.exceptions.ConnectionError as errc:
    LOGGER.error(
        "Status request failed with Error Connecting %s URL of request was %s",
        errc,
        status_url
    )
    sys.exit(1)
except requests.exceptions.Timeout as errt:
    LOGGER.error(
        "Status request failed with Timeout Error %s URL of request was %s",
        errt,
        status_url
    )
    sys.exit(1)
except requests.exceptions.RequestException as err:
    LOGGER.error(
        "Status request failed with error %s URL of request was %s",
        err,
        status_url
    )
    sys.exit(1)

# Check the status response from the server
if resp.status_code == 200:
    try:
        status_response = json.loads(resp.text)
    except json.JSONDecodeError as err:
        LOGGER.error(
            "Status request did not return the proper content %s "
            "URL of request was %s",
            err.msg,
            status_url
        )
        sys.exit(1)

    # Check the status code returned from the server
    # print("\n  Workflow status is {}".format(status_response["status"]))
else:
    LOGGER.error(
        "Status request failed with an error code %s. "
        "URL of request was %s. The response from the server: %s",
        resp.status_code,
        status_url,
        resp.text,
    )
    sys.exit(1)

#
# Download metadata for the workflow. Using the metadata we will display
# the status of each task and provide instructions for downloading
# the log files or other outputs.
#
# The first step is to download the metadata. This is a json file
# which contains all information about the workflow run
metadata_url = "{}/api/workflows/v1/{}/metadata".format(
    cromwell_server_url, workflow_id
)
try:
    resp = requests.get(metadata_url, headers=status_headers)
    resp.raise_for_status()
except (
    requests.exceptions.HTTPError,
    requests.exceptions.ConnectionError,
    requests.exceptions.Timeout,
    requests.exceptions.RequestException,
) as err:
    LOGGER.error(
        "Download of workflow metadata has failed. We will upload any logs "
        "files which exist. The error was %s. URL of request was %s",
        err,
        metadata_url,
    )
    sys.exit(1)

# Check the status response from the server
if resp.status_code == 200:
    try:
        workflow_metadata = json.loads(resp.text)
    except json.JSONDecodeError as err:
        LOGGER.warn(
            "Download of workflow metadata has failed. We will upload any "
            "logs files which exist. The download request did not return "
            "the proper content %s. URL of request was %s",
            err.msg,
            metadata_url,
        )
        sys.exit(1)
else:
    LOGGER.error(
        "Download of workflow metadata has failed. We will upload any "
        "logs files which exist. "
        "Download of metadata request failed with an error code %s. "
        "URL of request was %s. The response from the server: %s",
        resp.status_code,
        metadata_url,
        resp.text,
    )
    sys.exit(1)

# Display a workflow status
print("=================================================")
print("=================================================")
print("Workflow Name: {}".format(workflow_metadata["workflowName"]))
print("Workflow ID: {}".format(workflow_id))
print("Workflow Status: {}".format(workflow_metadata["status"]))
print("Workflow call/task status is below:")
# TODO add start and stop times here

for call in sorted(workflow_metadata["calls"]):
    print("")
    print("  - {}".format(call.split(".")[1]))
    if len(workflow_metadata["calls"][call]) == 1:
        pad = "    "
    else:
        pad = "      "

    for inst in workflow_metadata["calls"][call]:
        # Convert start and end dates to local timezone
        start_dto = convert_to_local_time(inst["start"])
        if "end" in inst:
            end_dto = convert_to_local_time(inst["end"])
        else:
            end_dto = ""

        if str(inst["shardIndex"]) != "-1":
            print("    - Run #{}".format(inst["shardIndex"]))

        if inst["callCaching"]["hit"] is True:
            callCaching_str = "[Results from cache]"
        else:
            callCaching_str = ""

        print("{}- Status={} {}".format(
            pad, inst["executionStatus"], callCaching_str
        ))
        print(
            "{}- Start time: {}  |  End time: {}".format(
                pad, start_dto, end_dto
            )
        )

        # Output further information about call
        if "download_input_file" in call:
            print("{}- Downloaded file: {}".format(
                pad, os.path.basename(inst["outputs"]["downloaded_file"])
            ))
        elif call.split(".")[1] == "encyclopedia" or call.split(".")[1] == "encyclopedia_wide":
            print("{}- Input file: {}".format(
                pad, inst["inputs"]["local_input_name"]
            ))


# Output the metadata as a JSON file. Use this for testing
# wf_exec_fh = open("./metadata.json", "w")
# wf_exec_fh.write(json.dumps(workflow_metadata, indent=4))
